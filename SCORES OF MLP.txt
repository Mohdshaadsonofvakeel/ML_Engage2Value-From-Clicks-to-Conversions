# === Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Garbage Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Columns ===
drop_cols = train.columns[train.nunique() <= 1].tolist()
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()
drop_cols += ['userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
              'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
              'device.mobileInputSelector', 'device.mobileDeviceBranding',
              'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',
              'device.operatingSystemVersion', 'socialEngagementType', 'browserMajor',
              'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
              'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
              'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent']
train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Parse Numeric and Cap Outliers ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Basic Behavioral Features ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)
train['page_x_hits'] = train['pageViews'] * train['totalHits']
test['page_x_hits'] = test['pageViews'] * test['totalHits']
train['hits_ratio'] = train['totalHits'] / (train['sessionNumber'] + 1)
test['hits_ratio'] = test['totalHits'] / (test['sessionNumber'] + 1)
train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)
if 'totals.bounces' in train.columns:
    train['is_bounce'] = (train['totals.bounces'] == 1).astype(int)
    test['is_bounce'] = (test['totals.bounces'] == 1).astype(int)
else:
    train['is_bounce'] = 0
    test['is_bounce'] = 0

# === Time Features ===
train['hour'] = pd.to_datetime(train['sessionStart'], unit='s', errors='coerce').dt.hour
train['weekday'] = pd.to_datetime(train['sessionStart'], unit='s', errors='coerce').dt.weekday
train['is_weekend'] = train['weekday'].isin([5, 6]).astype(int)
test['hour'] = pd.to_datetime(test['sessionStart'], unit='s', errors='coerce').dt.hour
test['weekday'] = pd.to_datetime(test['sessionStart'], unit='s', errors='coerce').dt.weekday
test['is_weekend'] = test['weekday'].isin([5, 6]).astype(int)
train['hour_x_hits'] = train['hour'] * train['totalHits']
test['hour_x_hits'] = test['hour'] * test['totalHits']
train['is_peak_hour'] = train['hour'].isin([17, 18, 19, 20, 21, 22]).astype(int)
test['is_peak_hour'] = test['hour'].isin([17, 18, 19, 20, 21, 22]).astype(int)

# === Device Flag ===
train['is_mobile'] = (train['device.isMobile'] == True).astype(int)
test['is_mobile'] = (test['device.isMobile'] == True).astype(int)

# === Traffic Quality ===
high_quality_mediums = ['(none)', 'cpm', 'cpc']
train['is_high_quality_medium'] = train['trafficSource.medium'].isin(high_quality_mediums).astype(int)
test['is_high_quality_medium'] = test['trafficSource.medium'].isin(high_quality_mediums).astype(int)

# === OS-Browser Combo Flags ===
train['os_browser'] = train['os'].astype(str) + ' - ' + train['browser'].astype(str)
test['os_browser'] = test['os'].astype(str) + ' - ' + test['browser'].astype(str)
top_combos = ['Macintosh - Chrome', 'Windows - Chrome', 'Chrome OS - Chrome']
for combo in top_combos:
    cname = 'combo_' + combo.lower().replace(' ', '_').replace('-', '_')
    train[cname] = (train['os_browser'] == combo).astype(int)
    test[cname] = (test['os_browser'] == combo).astype(int)
train.drop(columns=['os_browser'], inplace=True)
test.drop(columns=['os_browser'], inplace=True)

# === Frequency Encoding ===
freq_cols = ['geoNetwork.continent', 'geoNetwork.subContinent', 'trafficSource.medium', 'browser', 'os',
             'geoCluster', 'locationCountry', 'userChannel']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

# === Country Avg Hits ===
avg_hits_by_country = train.groupby('locationCountry')['totalHits'].mean().to_dict()
train['country_avg_hits'] = train['locationCountry'].map(avg_hits_by_country)
test['country_avg_hits'] = test['locationCountry'].map(avg_hits_by_country).fillna(0)

# === Final cleanup ===
train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Prepare Final Train/Test ===
y = train['purchaseValue']
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Validation Split ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train XGBoost Model ===
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.85,
    colsample_bytree=0.7,
    gamma=0.01,
    early_stopping_rounds=50,
    tree_method='hist',
    random_state=42,
    verbosity=0
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\nâœ… Validation RÂ²:", round(r2_score(y_val, val_preds), 4))

# === Predict and Save Submission ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, 1.5e10)
submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
print("\nðŸ“¦ submission.csv saved.")
/tmp/ipykernel_13/3545080814.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/3545080814.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
âœ… Validation RÂ²: 0.6127

ðŸ“¦ submission.csv saved.   GIVING .51 ON KAGGLE 

# === Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Garbage Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Columns ===
drop_cols = train.columns[train.nunique() <= 1].tolist()
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()
drop_cols += ['userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
              'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
              'device.mobileInputSelector', 'device.mobileDeviceBranding',
              'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',
              'device.operatingSystemVersion', 'socialEngagementType', 'browserMajor',
              'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
              'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
              'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent']
train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Clip and Parse ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)
    df['is_peak_hour'] = ts.dt.hour.isin([17, 18, 19, 20, 21, 22]).astype(int)

extract_time(train)
extract_time(test)

# === Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)
train['page_x_hits'] = train['pageViews'] * train['totalHits']
test['page_x_hits'] = test['pageViews'] * test['totalHits']
train['hits_ratio'] = train['totalHits'] / (train['sessionNumber'] + 1)
test['hits_ratio'] = test['totalHits'] / (test['sessionNumber'] + 1)
train['hour_x_hits'] = train['hour'] * train['totalHits']
test['hour_x_hits'] = test['hour'] * test['totalHits']

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

if 'totals.bounces' in train.columns:
    train['is_bounce'] = (train['totals.bounces'] == 1).astype(int)
    test['is_bounce'] = (test['totals.bounces'] == 1).astype(int)
else:
    train['is_bounce'] = 0
    test['is_bounce'] = 0

high_quality_mediums = ['(none)', 'cpm', 'cpc']
train['is_high_quality_medium'] = train['trafficSource.medium'].isin(high_quality_mediums).astype(int)
test['is_high_quality_medium'] = test['trafficSource.medium'].isin(high_quality_mediums).astype(int)

# === Frequency Encoding ===
freq_cols = ['geoNetwork.continent', 'geoNetwork.subContinent', 'trafficSource.medium', 'browser', 'os', 'userChannel']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

# === Drop non-numeric ===
train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Select Features ===
final_features = [
    'pageViews', 'totalHits', 'sessionNumber', 'hour', 'weekday',
    'hits_per_pageview', 'page_x_hits', 'hits_ratio', 'hour_x_hits',
    'is_repeating_user', 'is_bounce', 'is_weekend', 'is_peak_hour',
    'is_high_quality_medium',
    'geoNetwork.continent_freq', 'geoNetwork.subContinent_freq',
    'trafficSource.medium_freq', 'browser_freq', 'os_freq', 'userChannel_freq'
]

X = train[final_features].copy()
X_test = test[final_features].copy()
y = train['purchaseValue']

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Split ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train XGBoost Model (Optimized) ===
model = XGBRegressor(
    n_estimators=2000,
    learning_rate=0.03,
    max_depth=6,
    subsample=0.85,
    colsample_bytree=0.65,
    gamma=1.0,
    reg_alpha=10,
    reg_lambda=50,
    min_child_weight=5,
    tree_method='hist',
    early_stopping_rounds=50,
    random_state=42,
    verbosity=0
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\nâœ… Validation RÂ²:", round(r2_score(y_val, val_preds), 4))

# === Predict Test and Clip Output ===
clip_val = y.quantile(0.995)
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, clip_val)

# === Save Submission ===
submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
print("\nðŸ“¦ submission.csv saved with clipped predictions.")
/tmp/ipykernel_13/398871421.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/398871421.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
âœ… Validation RÂ²: 0.4484

ðŸ“¦ submission.csv saved with clipped predictions.    ON KAGGLE .16 SCORE 


# === Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Garbage Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Columns ===
drop_cols = train.columns[train.nunique() <= 1].tolist()
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()

drop_cols += ['userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
              'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
              'device.mobileInputSelector', 'device.mobileDeviceBranding',
              'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',
              'device.operatingSystemVersion', 'socialEngagementType', 'browserMajor',
              'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
              'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
              'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent']

train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Numeric Clean + Clip ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

if 'totals.bounces' in train.columns:
    train['is_bounce'] = (train['totals.bounces'] == 1).astype(int)
    test['is_bounce'] = (test['totals.bounces'] == 1).astype(int)
else:
    train['is_bounce'] = 0
    test['is_bounce'] = 0

# === Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['month'] = ts.dt.month
    df['year'] = ts.dt.year
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)
    df['is_peak_hour'] = df['hour'].isin([17, 18, 19, 20, 21, 22]).astype(int)

extract_time(train)
extract_time(test)

# === Traffic Medium Quality ===
high_quality_mediums = ['(none)', 'cpm', 'cpc']
train['is_high_quality_medium'] = train['trafficSource.medium'].isin(high_quality_mediums).astype(int)
test['is_high_quality_medium'] = test['trafficSource.medium'].isin(high_quality_mediums).astype(int)

# === OS-Browser Combo Flags ===
train['os_browser'] = train['os'].astype(str) + ' - ' + train['browser'].astype(str)
test['os_browser'] = test['os'].astype(str) + ' - ' + test['browser'].astype(str)

top_combos = ['Macintosh - Chrome', 'Windows - Chrome', 'Chrome OS - Chrome']
for combo in top_combos:
    cname = 'combo_' + combo.lower().replace(' ', '_').replace('-', '_')
    train[cname] = (train['os_browser'] == combo).astype(int)
    test[cname] = (test['os_browser'] == combo).astype(int)

train.drop(columns=['os_browser'], inplace=True)
test.drop(columns=['os_browser'], inplace=True)

# === Frequency Encoding ===
freq_cols = ['geoNetwork.continent', 'geoNetwork.subContinent', 'trafficSource.medium', 'browser', 'os',
             'geoCluster', 'locationCountry', 'userChannel']

for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Target + Final Feature Interactions ===
y = train['purchaseValue']
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]  # align columns

X['page_x_hits'] = X['pageViews'] * X['totalHits']
X['hits_ratio'] = X['totalHits'] / (X['sessionNumber'] + 1)
X['hour_x_hits'] = X['hour'] * X['totalHits']
X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
X_test['hits_ratio'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Split ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train XGBoost Model ===
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.85,
    colsample_bytree=0.7,
    gamma=0.01,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50,
    tree_method='hist'
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\nâœ… Validation RÂ²:", round(r2_score(y_val, val_preds), 4))

# === Predict Test ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, None)

# === Save Submission ===
submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
print("\nðŸ“¦ submission.csv saved.")
/tmp/ipykernel_13/2695899413.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/2695899413.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/2695899413.py:107: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
/tmp/ipykernel_13/2695899413.py:108: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hits_ratio'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
/tmp/ipykernel_13/2695899413.py:109: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
âœ… Validation RÂ²: 0.6155

ðŸ“¦ submission.csv saved.   ON KAGGLE .51 


# === Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Garbage Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Columns ===
drop_cols = train.columns[train.nunique() <= 1].tolist()
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()

drop_cols += ['userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
              'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
              'device.mobileInputSelector', 'device.mobileDeviceBranding',
              'device.mobileDeviceMarketingName', 'device.mobileDeviceModel',
              'device.operatingSystemVersion', 'socialEngagementType', 'browserMajor',
              'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
              'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
              'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent']

train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Numeric Clean + Clip ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

if 'totals.bounces' in train.columns:
    train['is_bounce'] = (train['totals.bounces'] == 1).astype(int)
    test['is_bounce'] = (test['totals.bounces'] == 1).astype(int)
else:
    train['is_bounce'] = 0
    test['is_bounce'] = 0


# === Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['month'] = ts.dt.month
    df['year'] = ts.dt.year
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)

extract_time(train)
extract_time(test)

# === Frequency Encoding ===
freq_cols = ['geoNetwork.continent', 'geoNetwork.subContinent', 'trafficSource.medium', 'browser', 'os',
             'geoCluster', 'locationCountry', 'userChannel']

for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Target + Final Feature Interactions ===
y = train['purchaseValue']
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]  # align columns

X['page_x_hits'] = X['pageViews'] * X['totalHits']
X['hits_ratio'] = X['totalHits'] / (X['sessionNumber'] + 1)
X['hour_x_hits'] = X['hour'] * X['totalHits']
X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
X_test['hits_ratio'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Split ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train XGBoost Model ===
model = XGBRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.85,
    colsample_bytree=0.7,
    gamma=0.01,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50,
    tree_method='hist'
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\nâœ… Validation RÂ²:", round(r2_score(y_val, val_preds), 4))

# === Predict Test ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, None)

# === Save Submission ===
submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
print("\nðŸ“¦ submission.csv saved.")
/tmp/ipykernel_13/112669660.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/112669660.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/112669660.py:89: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
/tmp/ipykernel_13/112669660.py:90: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hits_ratio'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
/tmp/ipykernel_13/112669660.py:91: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
âœ… Validation RÂ²: 0.5846

ðŸ“¦ submission.csv saved.    ON KAGGLE .50 


# === FinalNotebook: XGBoost Pipeline Optimized for Kaggle ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Missing Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Useless Columns ===
drop_cols = [
    'userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
    'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
    'device.mobileInputSelector', 'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName',
    'device.mobileDeviceModel', 'device.operatingSystemVersion', 'socialEngagementType',
    'browserMajor', 'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent'
]
drop_cols += [col for col in train.columns if train[col].nunique() <= 1]
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()

train.drop(columns=drop_cols, inplace=True)
test.drop(columns=drop_cols, inplace=True)

# === Cap Outliers ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Basic Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

train['is_bounce'] = (train['totals.bounces'] == 1).astype(int) if 'totals.bounces' in train.columns else 0
test['is_bounce'] = (test['totals.bounces'] == 1).astype(int) if 'totals.bounces' in test.columns else 0

# === Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['month'] = ts.dt.month
    df['year'] = ts.dt.year
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)

extract_time(train)
extract_time(test)

# === Frequency Encoding for High-Card Columns ===
freq_cols = ['geoCluster', 'geoNetwork.city', 'trafficSource.keyword', 'locationCountry']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Target + Feature Engineering ===
y = train['purchaseValue'].clip(upper=np.percentile(train['purchaseValue'][train['purchaseValue'] > 0], 99))
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]

# Interaction Features
X['page_x_hits'] = X['pageViews'] * X['totalHits']
X['hour_x_hits'] = X['hour'] * X['totalHits']
X['hits_divided_session'] = X['totalHits'] / (X['sessionNumber'] + 1)
X['intent_score'] = X['is_repeating_user'] + X['pageViews']

X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
X_test['hits_divided_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
X_test['intent_score'] = X_test['is_repeating_user'] + X_test['pageViews']

# Drop high-overfit features
X.drop(columns=['weekday_x_hour', 'pageViews_squared'], errors='ignore', inplace=True)
X_test.drop(columns=['weekday_x_hour', 'pageViews_squared'], errors='ignore', inplace=True)

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Split (15% val for early stopping) ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train Model with Balanced Regularization ===
model = XGBRegressor(
    n_estimators=2500,
    learning_rate=0.015,
    max_depth=6,
    subsample=0.75,
    colsample_bytree=0.75,
    reg_alpha=1.0,
    reg_lambda=2.0,
    gamma=1.0,
    early_stopping_rounds=50,
    tree_method='hist',
    random_state=42,
    verbosity=0
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\u2705 Validation R\u00b2:", r2_score(y_val, val_preds))

# === Predict & Submit ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/4231205914.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/4231205914.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/4231205914.py:85: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
/tmp/ipykernel_13/4231205914.py:86: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
/tmp/ipykernel_13/4231205914.py:87: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hits_divided_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
/tmp/ipykernel_13/4231205914.py:88: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['intent_score'] = X_test['is_repeating_user'] + X_test['pageViews']
/tmp/ipykernel_13/4231205914.py:92: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test.drop(columns=['weekday_x_hour', 'pageViews_squared'], errors='ignore', inplace=True)
âœ… Validation RÂ²: 0.45800665716531863  
 ONKAGGLE .20 

# === FinalNotebook: XGBoost Pipeline Optimized for Kaggle ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Missing Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Useless Columns ===
drop_cols = [
    'userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
    'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
    'device.mobileInputSelector', 'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName',
    'device.mobileDeviceModel', 'device.operatingSystemVersion', 'socialEngagementType',
    'browserMajor', 'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent'
]
drop_cols += [col for col in train.columns if train[col].nunique() <= 1]
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()

train.drop(columns=drop_cols, inplace=True)
test.drop(columns=drop_cols, inplace=True)

# === Cap Outliers ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Basic Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

train['is_bounce'] = (train['totals.bounces'] == 1).astype(int) if 'totals.bounces' in train.columns else 0
test['is_bounce'] = (test['totals.bounces'] == 1).astype(int) if 'totals.bounces' in test.columns else 0

# === Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['month'] = ts.dt.month
    df['year'] = ts.dt.year
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)

extract_time(train)
extract_time(test)

# === Frequency Encoding for High-Card Columns ===
freq_cols = ['geoCluster', 'geoNetwork.city', 'trafficSource.keyword', 'locationCountry']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Target + Feature Engineering ===
y = train['purchaseValue'].clip(upper=np.percentile(train['purchaseValue'][train['purchaseValue'] > 0], 99))
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]

# Interaction Features
X['page_x_hits'] = X['pageViews'] * X['totalHits']
X['hour_x_hits'] = X['hour'] * X['totalHits']
X['hits_divided_session'] = X['totalHits'] / (X['sessionNumber'] + 1)
X['intent_score'] = X['is_repeating_user'] + X['pageViews']

X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
X_test['hits_divided_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
X_test['intent_score'] = X_test['is_repeating_user'] + X_test['pageViews']

# Drop high-overfit features
X.drop(columns=['weekday_x_hour', 'pageViews_squared'], errors='ignore', inplace=True)
X_test.drop(columns=['weekday_x_hour', 'pageViews_squared'], errors='ignore', inplace=True)

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Split (20% val for early stopping) ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train Model with Stronger Regularization ===
model = XGBRegressor(
    n_estimators=2000,
    learning_rate=0.01,
    max_depth=4,
    subsample=0.6,
    colsample_bytree=0.6,
    reg_alpha=2.0,
    reg_lambda=4.0,
    gamma=2.0,
    early_stopping_rounds=50,
    tree_method='hist',
    random_state=42,
    verbosity=0
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\u2705 Validation R\u00b2:", r2_score(y_val, val_preds))

# === Predict & Submit ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/4128559317.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/4128559317.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/4128559317.py:85: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
/tmp/ipykernel_13/4128559317.py:86: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
/tmp/ipykernel_13/4128559317.py:87: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hits_divided_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
/tmp/ipykernel_13/4128559317.py:88: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['intent_score'] = X_test['is_repeating_user'] + X_test['pageViews']
/tmp/ipykernel_13/4128559317.py:92: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test.drop(columns=['weekday_x_hour', 'pageViews_squared'], errors='ignore', inplace=True)
âœ… Validation RÂ²: 0.3199536205803952   ON KAGGLE .12


# === FinalNotebook: XGBoost Pipeline Optimized for Kaggle ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Missing Values ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Drop Useless Columns ===
drop_cols = [
    'userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
    'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
    'device.mobileInputSelector', 'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName',
    'device.mobileDeviceModel', 'device.operatingSystemVersion', 'socialEngagementType',
    'browserMajor', 'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent'
]
drop_cols += [col for col in train.columns if train[col].nunique() <= 1]
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()

train.drop(columns=drop_cols, inplace=True)
test.drop(columns=drop_cols, inplace=True)

# === Cap Outliers ===
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Basic Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

train['is_bounce'] = (train['totals.bounces'] == 1).astype(int) if 'totals.bounces' in train.columns else 0
test['is_bounce'] = (test['totals.bounces'] == 1).astype(int) if 'totals.bounces' in test.columns else 0

# === Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['month'] = ts.dt.month
    df['year'] = ts.dt.year
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)

extract_time(train)
extract_time(test)

# === Frequency Encoding for High-Card Columns ===
freq_cols = ['geoCluster', 'geoNetwork.city', 'trafficSource.keyword', 'locationCountry']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Target + Feature Engineering ===
y = train['purchaseValue'].clip(upper=np.percentile(train['purchaseValue'][train['purchaseValue'] > 0], 99))
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]

# Interaction Features
X['page_x_hits'] = X['pageViews'] * X['totalHits']
X['hour_x_hits'] = X['hour'] * X['totalHits']
X['weekday_x_hour'] = X['weekday'] * X['hour']
X['pageViews_squared'] = X['pageViews'] ** 2
X['hits_divided_session'] = X['totalHits'] / (X['sessionNumber'] + 1)

X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
X_test['weekday_x_hour'] = X_test['weekday'] * X_test['hour']
X_test['pageViews_squared'] = X_test['pageViews'] ** 2
X_test['hits_divided_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)

# === Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Stratified Split (10% val for early stopping) ===
stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

# === Train Model ===
model = XGBRegressor(
    n_estimators=3000,
    learning_rate=0.02,
    max_depth=6,
    subsample=0.7,
    colsample_bytree=0.7,
    reg_alpha=1.0,
    reg_lambda=2.0,
    gamma=1.0,
    early_stopping_rounds=50,
    random_state=42,
    verbosity=0,
    tree_method='hist'
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("\u2705 Validation R\u00b2:", r2_score(y_val, val_preds))

# === Predict & Submit ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/1655625053.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/1655625053.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/1655625053.py:86: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
/tmp/ipykernel_13/1655625053.py:87: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
/tmp/ipykernel_13/1655625053.py:88: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['weekday_x_hour'] = X_test['weekday'] * X_test['hour']
/tmp/ipykernel_13/1655625053.py:89: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['pageViews_squared'] = X_test['pageViews'] ** 2
/tmp/ipykernel_13/1655625053.py:90: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hits_divided_session'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
âœ… Validation RÂ²: 0.5282664653666012    ON KAGGLE .22 SCORE 


# === Cell 1: Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from lightgbm import LGBMClassifier
from xgboost import XGBRegressor

# === Cell 2: Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Cell 3: Clean Garbage ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Cell 4: Drop Useless Columns ===
drop_cols = [
    'userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
    'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
    'device.mobileInputSelector', 'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName',
    'device.mobileDeviceModel', 'device.operatingSystemVersion', 'socialEngagementType',
    'browserMajor', 'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent'
]
drop_cols += [col for col in train.columns if train[col].nunique() <= 1]
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()
train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Cell 5: Advanced Feature Engineering ===
def engineer_advanced_features(train, test):
    train['is_train'] = 1
    test['is_train'] = 0
    full = pd.concat([train, test], sort=False).reset_index(drop=True)

    ts = pd.to_datetime(full['sessionStart'], unit='s', errors='coerce')
    full['hour'] = ts.dt.hour
    full['weekday'] = ts.dt.weekday
    full['month'] = ts.dt.month
    full['year'] = ts.dt.year
    full['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)
    full['month_day'] = ts.dt.strftime('%m-%d')

    full['pageViews'] = pd.to_numeric(full['pageViews'], errors='coerce').fillna(0).clip(0, 100)
    full['totalHits'] = pd.to_numeric(full['totalHits'], errors='coerce').fillna(0).clip(0, 100)
    full['hits_per_page'] = full['totalHits'] / (full['pageViews'] + 1)
    full['page_x_hits'] = full['pageViews'] * full['totalHits']
    full['hour_x_hits'] = full['hour'] * full['totalHits']
    full['page_bin'] = pd.qcut(full['pageViews'], q=5, labels=False, duplicates='drop')
    full['hit_bin'] = pd.cut(full['totalHits'], bins=[-1, 1, 3, 10, 30, 100], labels=False)

    buyers = train[train['purchaseValue'] > 0].copy()
    group_cols = ['locationCountry', 'os', 'browser', 'geoCluster', 'userChannel', 'hour']
    for col in group_cols:
        if col in buyers.columns and col in full.columns:
            buyer_avg = buyers.groupby(col)['purchaseValue'].mean()
            full[f'{col}_buyer_mean'] = full[col].map(buyer_avg).fillna(0)

    if 'purchaseValue' in train.columns:
        train_ts = pd.to_datetime(train['sessionStart'], unit='s', errors='coerce')
        train['month_day'] = train_ts.dt.strftime('%m-%d')
        drift_avg = train.groupby('month_day')['purchaseValue'].mean()
        full['daily_drift_score'] = full['month_day'].map(drift_avg).fillna(0)

    full['is_repeating_user'] = (full['sessionNumber'] > 1).astype(int)
    if 'totals.bounces' in full.columns:
        full['is_bounce'] = (full['totals.bounces'] == 1).astype(int)
    else:
        full['is_bounce'] = 0

    full.drop(columns=['sessionStart', 'month_day'], inplace=True, errors='ignore')
    train = full[full['is_train'] == 1].drop(columns=['is_train'])
    test = full[full['is_train'] == 0].drop(columns=['is_train'])
    return train, test

train, test = engineer_advanced_features(train, test)

# === Cell 6: Frequency Encode Selected Categorical Columns ===
freq_cols = ['geoNetwork.continent', 'geoNetwork.subContinent', 'trafficSource.medium', 'browser', 'os', 'geoCluster', 'locationCountry', 'userChannel']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Cell 7: Split Features and Target ===
y = train['purchaseValue']
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]

# === Cell 8: Impute and Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Cell 9: 2-Stage Model ===
y_binary = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, y_binary):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    y_train_bin, y_val_bin = y_binary.iloc[train_idx], y_binary.iloc[val_idx]

clf = LGBMClassifier(n_estimators=300, max_depth=7, random_state=42, class_weight='balanced')
clf.fit(X_train, y_train_bin)
prob_val = clf.predict_proba(X_val)[:, 1]
threshold = 0.10
likely_idx = prob_val > threshold
X_val_likely = X_val[likely_idx]
y_val_likely = y_val.iloc[likely_idx]

regressor = XGBRegressor(
    n_estimators=2000,
    learning_rate=0.05,
    max_depth=10,
    subsample=0.9,
    colsample_bytree=0.7,
    min_child_weight=5,
    reg_alpha=10,
    gamma=0.01,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50,
    tree_method='hist'
)

regressor.fit(X_train, y_train, eval_set=[(X_val_likely, y_val_likely)], verbose=False)
val_preds_likely = regressor.predict(X_val_likely)
val_preds_final = np.zeros_like(y_val)
val_preds_final[likely_idx] = val_preds_likely
print("âœ… 2-Stage Validation RÂ²:", r2_score(y_val, val_preds_final))

# === Cell 10: Test Predictions ===
prob_test = clf.predict_proba(X_test_scaled)[:, 1]
likely_test_idx = prob_test > threshold
test_preds = np.zeros(X_test_scaled.shape[0])
test_preds[likely_test_idx] = regressor.predict(X_test_scaled[likely_test_idx])
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/1109138781.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/1109138781.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
[LightGBM] [Info] Number of positive: 19188, number of negative: 73630
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013030 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 2006
[LightGBM] [Info] Number of data points in the train set: 92818, number of used features: 31
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
âœ… 2-Stage Validation RÂ²: 0.7080813946188851      ON KAGGLE .46 


# === Cell 1: Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.linear_model import LogisticRegression
from xgboost import XGBRegressor

# === Cell 2: Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Cell 3: Clean Garbage ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Cell 4: Drop Useless Columns ===
drop_cols = [
    'userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
    'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
    'device.mobileInputSelector', 'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName',
    'device.mobileDeviceModel', 'device.operatingSystemVersion', 'socialEngagementType',
    'browserMajor', 'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent'
]
drop_cols += [col for col in train.columns if train[col].nunique() <= 1]
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()
train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Cell 5: Advanced Feature Engineering ===
def engineer_advanced_features(train, test):
    train['is_train'] = 1
    test['is_train'] = 0
    full = pd.concat([train, test], sort=False).reset_index(drop=True)

    ts = pd.to_datetime(full['sessionStart'], unit='s', errors='coerce')
    full['hour'] = ts.dt.hour
    full['weekday'] = ts.dt.weekday
    full['month'] = ts.dt.month
    full['year'] = ts.dt.year
    full['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)
    full['month_day'] = ts.dt.strftime('%m-%d')

    full['pageViews'] = pd.to_numeric(full['pageViews'], errors='coerce').fillna(0).clip(0, 100)
    full['totalHits'] = pd.to_numeric(full['totalHits'], errors='coerce').fillna(0).clip(0, 100)
    full['hits_per_page'] = full['totalHits'] / (full['pageViews'] + 1)
    full['page_x_hits'] = full['pageViews'] * full['totalHits']
    full['hour_x_hits'] = full['hour'] * full['totalHits']
    full['page_bin'] = pd.qcut(full['pageViews'], q=5, labels=False, duplicates='drop')
    full['hit_bin'] = pd.cut(full['totalHits'], bins=[-1, 1, 3, 10, 30, 100], labels=False)

    buyers = train[train['purchaseValue'] > 0].copy()
    group_cols = ['locationCountry', 'os', 'browser', 'geoCluster', 'userChannel', 'hour']
    for col in group_cols:
        if col in buyers.columns and col in full.columns:
            buyer_avg = buyers.groupby(col)['purchaseValue'].mean()
            full[f'{col}_buyer_mean'] = full[col].map(buyer_avg).fillna(0)

    if 'purchaseValue' in train.columns:
        train_ts = pd.to_datetime(train['sessionStart'], unit='s', errors='coerce')
        train['month_day'] = train_ts.dt.strftime('%m-%d')
        drift_avg = train.groupby('month_day')['purchaseValue'].mean()
        full['daily_drift_score'] = full['month_day'].map(drift_avg).fillna(0)

    full['is_repeating_user'] = (full['sessionNumber'] > 1).astype(int)
    if 'totals.bounces' in full.columns:
        full['is_bounce'] = (full['totals.bounces'] == 1).astype(int)
    else:
        full['is_bounce'] = 0

    full.drop(columns=['sessionStart', 'month_day'], inplace=True, errors='ignore')
    train = full[full['is_train'] == 1].drop(columns=['is_train'])
    test = full[full['is_train'] == 0].drop(columns=['is_train'])
    return train, test

train, test = engineer_advanced_features(train, test)

# === Cell 6: Frequency Encode Selected Categorical Columns ===
freq_cols = ['geoNetwork.continent', 'geoNetwork.subContinent', 'trafficSource.medium', 'browser', 'os', 'geoCluster', 'locationCountry', 'userChannel']
for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Cell 7: Split Features and Target ===
y = train['purchaseValue']
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]

# === Cell 8: Impute and Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Cell 9: 2-Stage Model ===
y_binary = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, y_binary):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    y_train_bin, y_val_bin = y_binary.iloc[train_idx], y_binary.iloc[val_idx]

clf = LogisticRegression(max_iter=300, class_weight='balanced', solver='liblinear')
clf.fit(X_train, y_train_bin)
prob_val = clf.predict_proba(X_val)[:, 1]
threshold = 0.2
likely_idx = prob_val > threshold
X_val_likely = X_val[likely_idx]
y_val_likely = y_val.iloc[likely_idx]

regressor = XGBRegressor(
    n_estimators=2000,
    learning_rate=0.05,
    max_depth=10,
    subsample=0.9,
    colsample_bytree=0.7,
    min_child_weight=5,
    reg_alpha=10,
    gamma=0.01,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50,
    tree_method='hist'
)

regressor.fit(X_train, y_train, eval_set=[(X_val_likely, y_val_likely)], verbose=False)
val_preds_likely = regressor.predict(X_val_likely)
val_preds_final = np.zeros_like(y_val)
val_preds_final[likely_idx] = val_preds_likely
print("âœ… 2-Stage Validation RÂ²:", r2_score(y_val, val_preds_final))

# === Cell 10: Test Predictions ===
prob_test = clf.predict_proba(X_test_scaled)[:, 1]
likely_test_idx = prob_test > threshold
test_preds = np.zeros(X_test_scaled.shape[0])
test_preds[likely_test_idx] = regressor.predict(X_test_scaled[likely_test_idx])
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/2662557258.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/2662557258.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
âœ… 2-Stage Validation RÂ²: 0.7071098743841706    ON KAGGLE SCORE .57 

# === Cell 1: Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Cell 2: Load Data ===
train = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Cell 3: Clean Garbage ===
garbage = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train.replace(garbage, np.nan, inplace=True)
test.replace(garbage, np.nan, inplace=True)

# === Cell 4: Drop Columns ===
drop_cols = [
    'userId', 'sessionId', 'device.browserVersion', 'device.screenResolution',
    'device.screenColors', 'device.language', 'deviceType', 'device.browserSize',
    'device.mobileInputSelector', 'device.mobileDeviceBranding', 'device.mobileDeviceMarketingName',
    'device.mobileDeviceModel', 'device.operatingSystemVersion', 'socialEngagementType',
    'browserMajor', 'geoNetwork.networkLocation', 'trafficSource.adwordsClickInfo.isVideoAd',
    'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot',
    'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adContent'
]
drop_cols += [col for col in train.columns if train[col].nunique() <= 1]
drop_cols += train.columns[train.isna().mean() > 0.95].tolist()

train.drop(columns=drop_cols, inplace=True, errors='ignore')
test.drop(columns=drop_cols, inplace=True, errors='ignore')

# === Cell 5: Numeric Clean + Clip
for col in ['pageViews', 'totalHits']:
    train[col] = pd.to_numeric(train[col], errors='coerce').fillna(0).clip(0, 100)
    test[col] = pd.to_numeric(test[col], errors='coerce').fillna(0).clip(0, 100)

# === Cell 6: Feature Engineering ===
train['hits_per_pageview'] = train['totalHits'] / train['pageViews'].replace(0, np.nan)
test['hits_per_pageview'] = test['totalHits'] / test['pageViews'].replace(0, np.nan)

train['is_repeating_user'] = (train['sessionNumber'] > 1).astype(int)
test['is_repeating_user'] = (test['sessionNumber'] > 1).astype(int)

if 'totals.bounces' in train.columns:
    train['is_bounce'] = (train['totals.bounces'] == 1).astype(int)
    test['is_bounce'] = (test['totals.bounces'] == 1).astype(int)
else:
    train['is_bounce'] = 0
    test['is_bounce'] = 0

# === Cell 7: Time Features ===
def extract_time(df):
    ts = pd.to_datetime(df['sessionStart'], unit='s', errors='coerce')
    df['hour'] = ts.dt.hour
    df['weekday'] = ts.dt.weekday
    df['month'] = ts.dt.month
    df['year'] = ts.dt.year
    df['is_weekend'] = ts.dt.weekday.isin([5, 6]).astype(int)

extract_time(train)
extract_time(test)

# === Cell 8: Frequency Encoding ===
freq_cols = [
    'geoNetwork.continent', 'geoNetwork.subContinent',
    'trafficSource.medium', 'browser', 'os',
    'geoCluster', 'locationCountry', 'userChannel'
]

for col in freq_cols:
    if col in train.columns:
        freqs = train[col].value_counts().to_dict()
        train[col + '_freq'] = train[col].map(freqs).fillna(0)
        test[col + '_freq'] = test[col].map(freqs).fillna(0)

# Drop all remaining object columns
train.drop(columns=[col for col in train.columns if train[col].dtype == 'object'], inplace=True)
test.drop(columns=[col for col in test.columns if test[col].dtype == 'object'], inplace=True)

# === Cell 9: Train/Test Split ===
y = train['purchaseValue']
X = train.drop(columns=['purchaseValue'])
X_test = test[X.columns]  # align columns

X['page_x_hits'] = X['pageViews'] * X['totalHits']
X['hits_ratio'] = X['totalHits'] / (X['sessionNumber'] + 1)
X['hour_x_hits'] = X['hour'] * X['totalHits']
X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
X_test['hits_ratio'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']

# === Cell 10: Impute + Scale ===
imputer = SimpleImputer(strategy='median')
X_imp = imputer.fit_transform(X)
X_test_imp = imputer.transform(X_test)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Cell 11: Validation Split ===
from sklearn.model_selection import StratifiedShuffleSplit

stratify_target = (y > 0).astype(int)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, val_idx in sss.split(X_scaled, stratify_target):
    X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]


# === Cell 12: Train Model ===
model = XGBRegressor(
    n_estimators=2000,
    learning_rate=0.05063233,
    max_depth=10,
    subsample=0.88322903,
    colsample_bytree=0.66239781,
    gamma=0.01161672,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50,
    tree_method='hist'
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
val_preds = model.predict(X_val)
print("âœ… Validation RÂ²:", r2_score(y_val, val_preds))

# === Cell 13: Predict + Submit ===
test_preds = model.predict(X_test_scaled)
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/2773101075.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/2773101075.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test.replace(garbage, np.nan, inplace=True)
/tmp/ipykernel_13/2773101075.py:92: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['page_x_hits'] = X_test['pageViews'] * X_test['totalHits']
/tmp/ipykernel_13/2773101075.py:93: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hits_ratio'] = X_test['totalHits'] / (X_test['sessionNumber'] + 1)
/tmp/ipykernel_13/2773101075.py:94: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  X_test['hour_x_hits'] = X_test['hour'] * X_test['totalHits']
âœ… Validation RÂ²: 0.634545176334232    ON KAGGLE SCORE   .58



# === Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import Ridge
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import r2_score
from xgboost import XGBRegressor
from scipy.stats import uniform, randint

# === Load Data ===
train_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Step 1: Clean Missing Markers ===
missing_markers = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train_df.replace(missing_markers, np.nan, inplace=True)
test_df.replace(missing_markers, np.nan, inplace=True)

# === Step 2: Drop High-Missing & Constant Columns ===
missing_ratio = train_df.isna().mean()
drop_cols = missing_ratio[missing_ratio >= 0.7].index.tolist()
constant_cols = [col for col in train_df.columns if train_df[col].nunique() <= 1]
train = train_df.drop(columns=drop_cols + constant_cols, errors='ignore')
test = test_df.drop(columns=drop_cols + constant_cols, errors='ignore')

# === Step 3: Split Features and Target ===
X = train.drop(columns=['purchaseValue'])
y = train['purchaseValue']
X_test = test.copy()

# === Step 4: Combine for Consistent Encoding ===
combined = pd.concat([X, X_test], ignore_index=True)

# === Step 5: Label Encode with Unseen Handling ===
for col in combined.select_dtypes(include='object').columns:
    le = LabelEncoder()
    le.fit(X[col].astype(str))  # fit only on training data
    mapping = {label: idx for idx, label in enumerate(le.classes_)}
    combined[col] = combined[col].astype(str).map(mapping).fillna(-1).astype(int)

# === Step 6: Split Back ===
X_train_all = combined.iloc[:len(X), :]
X_test_all = combined.iloc[len(X):, :]

# === Step 7: Impute and Scale ===
imputer = SimpleImputer(strategy='median')
X_train_imp = imputer.fit_transform(X_train_all)
X_test_imp = imputer.transform(X_test_all)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)
X_test_scaled = scaler.transform(X_test_imp)

# === Step 8: Train/Val Split ===
X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=0.2, random_state=42)

# === Step 9: Ridge Feature Selection (Aggressive Filter) ===
ridge = Ridge(alpha=1.0).fit(X_train, y_train)
selector = SelectFromModel(ridge, threshold=1e6, prefit=True)
X_train_sel = selector.transform(X_train)
X_val_sel = selector.transform(X_val)
X_test_sel = selector.transform(X_test_scaled)

print("Selected Features:", X_train_sel.shape[1])

# === Step 10: RandomizedSearchCV for XGBoost Tuning ===
param_dist = {
    'n_estimators': randint(100, 500),
    'learning_rate': uniform(0.01, 0.3),
    'max_depth': randint(3, 10),
    'subsample': uniform(0.5, 0.5),
    'colsample_bytree': uniform(0.5, 0.5)
}

xgb_model = XGBRegressor(random_state=42, verbosity=0)

search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=10,
    scoring='r2',
    cv=3,
    verbose=0,
    n_jobs=2,
    random_state=42
)

search.fit(X_train_sel, y_train)
best_params = search.best_params_
print("Best Params:", best_params)

# === Step 11: Final Refit with Early Stopping ===
best_model = XGBRegressor(
    **best_params,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50
)

X_full = np.vstack([X_train_sel, X_val_sel])
y_full = pd.concat([y_train, y_val])

best_model.fit(
    X_full, y_full,
    eval_set=[(X_val_sel, y_val)],
    verbose=False
)

val_preds = best_model.predict(X_val_sel)
print("Final Validation RÂ²:", r2_score(y_val, val_preds))

# === Step 12: Predict Test and Save ===
test_preds = best_model.predict(X_test_sel)
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
print("Submission saved as submission.csv")
/tmp/ipykernel_13/416712254.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train_df.replace(missing_markers, np.nan, inplace=True)
/tmp/ipykernel_13/416712254.py:21: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test_df.replace(missing_markers, np.nan, inplace=True)
Selected Features: 16
Best Params: {'colsample_bytree': 0.5714334089609704, 'learning_rate': 0.20526654188465587, 'max_depth': 7, 'n_estimators': 357, 'subsample': 0.8609993861334124}
Final Validation RÂ²: 0.9636678689133444
Submission saved as submission.csv   ON KAGGLE SCORE .60


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Missing Values ===
missing_markers = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train_df.replace(missing_markers, np.nan, inplace=True)
test_df.replace(missing_markers, np.nan, inplace=True)

# Drop columns with >70% missing or only one unique value
drop_cols = train_df.columns[train_df.isnull().mean() > 0.7].tolist()
unique_cols = [col for col in train_df.columns if train_df[col].nunique() == 1]
drop_cols += unique_cols
train_df.drop(columns=drop_cols, inplace=True)
test_df.drop(columns=drop_cols, inplace=True)

# === Cap Outliers ===
for col in ['pageViews', 'totalHits']:
    if col in train_df.columns:
        train_df[col] = pd.to_numeric(train_df[col], errors='coerce').clip(0, 100)
        test_df[col] = pd.to_numeric(test_df[col], errors='coerce').clip(0, 100)

# === Frequency Encoding ===
cat_cols = train_df.select_dtypes(include='object').columns.tolist()
if 'purchaseValue' in cat_cols:
    cat_cols.remove('purchaseValue')

for col in cat_cols:
    freq_map = train_df[col].value_counts().to_dict()
    train_df[col] = train_df[col].map(freq_map).fillna(0)
    test_df[col] = test_df[col].map(freq_map).fillna(0)

# === Impute Numericals ===
num_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.tolist()
num_cols = [col for col in num_cols if col != 'purchaseValue']
imputer = SimpleImputer(strategy='median')
train_df[num_cols] = imputer.fit_transform(train_df[num_cols])
test_df[num_cols] = imputer.transform(test_df[num_cols])

# === Feature + Target Split ===
X = train_df.drop(columns=['purchaseValue'])
y = train_df['purchaseValue']
X_test = test_df[X.columns]  # align columns

# === Stratified Split ===
y_binary = (y > 0).astype(int)
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y_binary
)

# === Anti-Overfitting XGBoost Parameters ===
params = {
    'reg_alpha': 0.1,
    'reg_lambda': 2.0,
    'gamma': 1.0,
    'min_child_weight': 10,
    'max_depth': 3,
    'learning_rate': 0.005,
    'n_estimators': 5000,
    'subsample': 0.6,
    'colsample_bytree': 0.6,
    'colsample_bylevel': 0.8,
    'early_stopping_rounds': 50,
    'random_state': 42,
    'verbosity': 0
}

# === Train Model ===
model = XGBRegressor(**params)
model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)

# === Evaluation ===
train_r2 = model.score(X_train, y_train)
val_r2 = model.score(X_val, y_val)
print(f"Train RÂ²: {train_r2:.4f}")
print(f"Val RÂ²:   {val_r2:.4f}")
print(f"RÂ² Gap:   {train_r2 - val_r2:.4f}")

# === Predict for Submission ===
test_preds = model.predict(X_test)
test_preds = np.clip(test_preds, 0, None)

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)
/tmp/ipykernel_13/734680692.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train_df.replace(missing_markers, np.nan, inplace=True)
/tmp/ipykernel_13/734680692.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test_df.replace(missing_markers, np.nan, inplace=True)
/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal
  return op(a, b)
/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal
  return op(a, b)
/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater_equal
  return op(a, b)
/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less_equal
  return op(a, b)
Train RÂ²: 0.5358
Val RÂ²:   0.3879
RÂ² Gap:   0.1479    ON KAGGLE 0.09781




# === Imports ===
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import RidgeCV
from sklearn.metrics import r2_score
from xgboost import XGBRegressor

# === Load Data ===
train_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

# === Clean Missing Markers ===
missing_markers = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train_df.replace(missing_markers, np.nan, inplace=True)
test_df.replace(missing_markers, np.nan, inplace=True)

# === Drop columns with >70% missing or constant ===
missing_thresh = 0.7
missing_cols = train_df.columns[train_df.isna().mean() > missing_thresh].tolist()
constant_cols = [col for col in train_df.columns if train_df[col].nunique() <= 1]
drop_cols = list(set(missing_cols + constant_cols))
train_df.drop(columns=drop_cols, inplace=True)
test_df.drop(columns=drop_cols, inplace=True)

# === Target Separation ===
target = train_df['purchaseValue'].copy()
train_df.drop(columns=['purchaseValue'], inplace=True)

# === Combine Train and Test ===
all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)

# === Column Type Detection ===
num_cols = all_df.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = all_df.select_dtypes(include=['object']).columns.tolist()

# === Imputation ===
num_imputer = SimpleImputer(strategy='median')
cat_imputer = SimpleImputer(strategy='most_frequent')
all_df[num_cols] = num_imputer.fit_transform(all_df[num_cols])
all_df[cat_cols] = cat_imputer.fit_transform(all_df[cat_cols])

# === Cap outliers ===
for col in ['totalHits', 'pageviews']:
    if col in all_df.columns:
        all_df[col] = np.where(all_df[col] > 100, 100, all_df[col])

# === Frequency Encoding ===
for col in cat_cols:
    freq_map = all_df[col].value_counts().to_dict()
    all_df[col] = all_df[col].map(freq_map)

# === Smart Feature Engineering ===
if set(['totalHits', 'pageviews']).issubset(all_df.columns):
    all_df['hit_view_product'] = all_df['totalHits'] * all_df['pageviews']
if set(['visitNumber', 'newVisits']).issubset(all_df.columns):
    all_df['visit_efficiency'] = all_df['visitNumber'] / (all_df['newVisits'] + 1)
if set(['bounces', 'newVisits']).issubset(all_df.columns):
    all_df['bounce_ratio'] = all_df['bounces'] * all_df['newVisits']

# === Split Back ===
X_all = all_df.iloc[:len(target), :].copy()
X_test = all_df.iloc[len(target):, :].copy()
y = target.copy()

# === Stratified Split ===
stratify_labels = (y > 0).astype(int)
X_train, X_val, y_train, y_val = train_test_split(
    X_all, y, test_size=0.2, random_state=42, stratify=stratify_labels
)

# === Standard Scaling ===
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

# === XGBoost Model ===
xgb = XGBRegressor(
    n_estimators=250,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.9,
    colsample_bytree=0.8,
    gamma=0.1,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=30
)
xgb.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)
xgb_val_preds = xgb.predict(X_val_scaled)
xgb_test_preds = xgb.predict(X_test_scaled)

# === Ridge Model ===
ridge = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=5)
ridge.fit(X_train_scaled, y_train)
ridge_val_preds = ridge.predict(X_val_scaled)
ridge_test_preds = ridge.predict(X_test_scaled)

# === Blend Predictions ===
val_preds_blend = 0.6 * xgb_val_preds + 0.4 * ridge_val_preds
test_preds_blend = 0.6 * xgb_test_preds + 0.4 * ridge_test_preds

# === Validation Score ===
from sklearn.metrics import r2_score
val_r2 = r2_score(y_val, val_preds_blend)
print(f"Validation RÂ² Score: {val_r2:.4f}")

# === Final Submission ===
test_preds_blend = np.clip(test_preds_blend, 0, None)
submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds_blend
})
submission.to_csv("submission_v3.csv", index=False)
submission.head()
/tmp/ipykernel_13/1661549393.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train_df.replace(missing_markers, np.nan, inplace=True)
/tmp/ipykernel_13/1661549393.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test_df.replace(missing_markers, np.nan, inplace=True)
Validation RÂ² Score: 0.3923
id	purchaseValue
0	0	3.533837e+07
1	1	7.200005e+06
2	2	0.000000e+00
3	3	5.071985e+07
4	4	1.669843e+06
   ON KAGGLE SCORE .188

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import Ridge
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import r2_score
from xgboost import XGBRegressor
from scipy.stats import uniform, randint

# Load data
train_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv")
test_df = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv")

# Replace missing markers with NaN
missing_markers = ['Unknown', 'not available in demo dataset', '(not set)', '(not provided)']
train_df.replace(missing_markers, np.nan, inplace=True)
test_df.replace(missing_markers, np.nan, inplace=True)

# Drop columns with >= 70% missing or only one unique value
threshold = 0.7
missing_ratio = train_df.isna().mean()
drop_cols = missing_ratio[missing_ratio >= threshold].index.tolist()
n_uniq = [col for col in train_df.columns if train_df[col].nunique() == 1]
train = train_df.drop(columns=n_uniq + drop_cols, errors='ignore')
test = test_df.drop(columns=n_uniq + drop_cols, errors='ignore')

# Split features and target
X = train.drop(columns=['purchaseValue'])
y = train['purchaseValue']

# Combine for encoding, using cleaned test
combined = pd.concat([X, test], ignore_index=True)

# Encode object columns with LabelEncoder + unseen category handling
for col in combined.select_dtypes(include='object').columns:
    le = LabelEncoder()
    le.fit(X[col].astype(str))  # fit only on training data
    mapping = {label: idx for idx, label in enumerate(le.classes_)}
    combined[col] = combined[col].astype(str).map(mapping).fillna(-1).astype(int)

# Split back
X_train_all = combined.iloc[:len(X), :]
X_test_all = combined.iloc[len(X):, :]

# Imputation and scaling
imputer = SimpleImputer(strategy='median')
X_train_imp = imputer.fit_transform(X_train_all)
X_test_imp = imputer.transform(X_test_all)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imp)
X_test_scaled = scaler.transform(X_test_imp)

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=0.2, random_state=42)

# Feature selection with Ridge
ridge = Ridge(alpha=1.0).fit(X_train, y_train)
selector = SelectFromModel(ridge, threshold=1e6, prefit=True)
X_train_sel = selector.transform(X_train)
X_val_sel = selector.transform(X_val)
X_test_sel = selector.transform(X_test_scaled)

# Hyperparameter search
param_dist = {
    'n_estimators': randint(100, 500),
    'learning_rate': uniform(0.01, 0.3),
    'max_depth': randint(3, 10),
    'subsample': uniform(0.5, 0.5),
    'colsample_bytree': uniform(0.5, 0.5)
}


xgb_model = XGBRegressor(random_state=42, verbosity=0)

search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=10,
    scoring='r2',
    cv=3,
    verbose=0,
    n_jobs=2,
    random_state=42
)

search.fit(X_train_sel, y_train)

# Refit best model
best_params = search.best_params_

best_model = XGBRegressor(
    **best_params,
    random_state=42,
    verbosity=0,
    early_stopping_rounds=50
)

# Train on full train set with early stopping on val
X_full = np.vstack([X_train_sel, X_val_sel])
y_full = pd.concat([y_train, y_val])

best_model.fit(
    X_full, y_full,
    eval_set=[(X_val_sel, y_val)],
    verbose=False
)

# Evaluate
val_preds_refit = best_model.predict(X_val_sel)
print("Refit Tuned XGBoost RÂ²:", r2_score(y_val, val_preds_refit))
/tmp/ipykernel_13/139380719.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  train_df.replace(missing_markers, np.nan, inplace=True)
/tmp/ipykernel_13/139380719.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`
  test_df.replace(missing_markers, np.nan, inplace=True)
Refit Tuned XGBoost RÂ²: 0.9636678689133444
# === Prediction ===
test_preds = best_model.predict(X_test_sel)
test_preds = np.clip(test_preds, 0, None)

# === Submission ===
sample_submission = pd.read_csv("/kaggle/input/engage-2-value-from-clicks-to-conversions/sample_submission.csv")

submission = pd.DataFrame({
    'id': sample_submission['ID'],
    'purchaseValue': test_preds
})
submission.to_csv("submission.csv", index=False)  ON KAGGLE .60 SCORE 


